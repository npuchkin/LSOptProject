\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb,amsmath}
\usepackage{color}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\DeclareMathOperator{\trace}{Tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\rank}{rank}

\textheight=9.5in
\textwidth=7.5in
\oddsidemargin=-0.5in
\topmargin=-1in
\tolerance=2000
\flushbottom
\title{Large-Scale~Optimization~and~Applications~Course~Project\\
Combinatorial~optimization:~Max-Cut,~Min~Uncut~and~Sparsest~Cut~Problems}
\author{M.~Danilova, N.~Puchkin, A.~Shilova}
\date{28 March 2017}


\begin{document}
\maketitle

\section{Combinatorial optimization problems}

We consider three types of combinatorial optimization problems: Max-Cut, Min UnCut 
and uniform Sparsest Cut.
Detailed formulation of these problems is given below in corresponding subsections.
Now we only mention, that all these problems are NP-hard and cannot be solved 
efficiently if $P \neq NP$.



\subsection{Max-Cut problem}

The maximal cut problem or Max-Cut can be formulated as follows: given an undirected 
weighted graph $G = (V, E, W)$, where $V = \{1, \dots, n\}$ is a set of vertices, $E 
\subseteq V \times V$ is a set of edges and $W$ is a matrix of weights, one wants to find 
a partition $(S, V\backslash S)$ maximizing the sum of weights of edges in the cut.
This problem can be formulated as a combinatorial optimization
\begin{equation}
\label{maxcut}
	\begin{cases}
		\frac12 \sum\limits_{(i, j) \in E} w_{ij} ( 1 - x_i x_j ) \longrightarrow \max \\
		x_i \in \{ -1, 1 \}, \quad i = \overline{1, n}
	\end{cases}
\end{equation}
This problem is indeed Max-Cut, since the partition can be defined by the formula $S = 
\{ i : x_i \geq 0 \}$.
An edge $(i, j)$ belongs to cut if and only if $x_i x_j = -1$.
The Max-Cut problem \ref{maxcut} can be rewritten as follows:
\begin{equation}
\label{maxcut1}
	\begin{cases}
		\frac14 \vec 1^T W \vec 1 - \frac14 x^T W x \longrightarrow \max \\
		x \in \{-1, 1\}^n,
	\end{cases}
\end{equation}
where $\vec 1$ stands for the vector of ones.



\subsection{Min UnCut problem}

The minimal uncut problem is a complement of the Max-Cut problem and can be 
formulated as follows.
Given an undirected weighted graph $G = (V, E, W)$, where $V = \{1, \dots, n\}$ is a set 
of vertices, $E \subseteq V \times V$ is a set of edges and $W$ is a matrix of weights, 
one wants to find a partition $(S, V\backslash S)$ minimizing the sum of weights of 
edges in $S$ and $V \backslash S$.
In terms of optimization this problem can be formulated as follows:
\begin{equation}
\label{minuncut}
	\begin{cases}
		\frac14 \vec 1^T W \vec 1 + \frac14 x^T W x \longrightarrow \min \\
		x \in \{-1, 1\}^n
	\end{cases}
\end{equation}
Indeed, the sum of all edges in the graph $G$ is given by the formula $\frac12 \vec 1^T W 
\vec 1$. Subtracting the sum of weights of edges in the cut $\frac14 \vec 1^T W \vec 1 - 
\frac14 x^T W x$, one arrives at the formulation \ref{minuncut}.



\subsection{Sparsest Cut problem}

Finally, we give a formulation of the uniform Sparsest Cut problem.
Let $G = (V, E)$, $|V| = n$ stand for a undirected graph and let $S \subseteq V$.
Denote $E(S)$ a set of edges in $G$ belonging to the cut $(S, V \backslash S)$.
Then the sparsest cut problem can be formulated as follows:
\begin{equation}
\label{spcut}
	\frac{ |E(S)| }{ |S| \cdot |V \backslash S| } \rightarrow \min\limits_S
\end{equation}
Sparsest Cut is related to the following problem.
Denote $\varphi(S) = \frac{ |E(S)| }{ |S| }$.
The number $\varphi(S)$ is called a sparsity of the cut $S$.
The problem of sparsity minimization is formulated as follows:
\begin{equation}
\label{sparsity}
	\varphi(S) \frac{ |E(S)| }{ |S| } \longrightarrow \min\limits_{S : |S| \leq \frac n2}
\end{equation}
Since for every cut $S$, $|S| \leq \frac n2$, it holds	$\frac n2 \leq | V \backslash S| \leq 
n$, then obviously
\[
	 \frac{ \varphi(S) }n \leq \frac{ |E(S)| }{ |S| \cdot |V \backslash S| } \leq  \frac{ 
	 2\varphi(S) }n
\]
and the problem \ref{sparsity} can be considered as an upper-bound of the problem 
\ref{spcut} up to a factor $\frac 2n$.

Modify the problem \ref{spcut}.
Consider $x \in {-1, 1}^n$ and define $S = \{ i : x_i > 0 \}$.
Then the number of edges in the cut $|E(S)|$ can be found according to the formula 
\[
	| E(S) | = \frac14 \vec 1^T A \vec 1 - \frac14 x^T A x,
\]
where $A$ is an adjacency matrix of the graph $G$.
Now we find expressions for $|S|$ and $|V\backslash S|$.
Note, that
\[
	\sum\limits_{i=1}^n x_i = |S| - |V\backslash S|
\]
Then $|S|$ and $|V\backslash S|$ can be found from the system of equations
\[
	\begin{cases}
		|S| - |V\backslash S| = \sum\limits_{i=1}^n x_i\\
		|S| + |V\backslash S| = n
	\end{cases}
\]
Solving this system, one obtains
\begin{gather}
\label{1}
	|S| = \frac12 \left( n + \sum\limits_{i=1}^n x_i \right) \\
	|V| = \frac12 \left( n - \sum\limits_{i=1}^n x_i \right)
\end{gather}
Then the problem \ref{spcut} can be formulated as follows:
\begin{equation}
\label{spcut1}
	\begin{cases}
		\frac{ \vec 1^T A \vec 1 - \frac14 x^T A x }{ n^2 - \left( \sum\limits_{i=1}^n x_i 
		\right)^2 } 
		\longrightarrow \min \\
		x \in \{-1, 1\}^n
	\end{cases}
\end{equation} 

	
		
\section{Greedy algorithm}

In this section we provide a naive greedy algorithm of combinatorial optimization.
Advantage of this method is simple implementation.
The pseudocode is given below

\begin{algorithm}[H]
	\caption{Naive greedy algorithm}
	\label{greedy}
	\begin{algorithmic}[1]
		\Require $F(\cdot)$ -- a functional to minimize
		\Ensure $x_{greedy}$, $F(x_{greedy})$ -- optimal values
		\State Initialize $x$ randomly
		\While{True}
			\State Get a neighbourhood $\mathcal N(x)$ of the point $x$
			\State Find $y^* = \arg\min\limits_{y \in \mathcal N(x) \backslash \{x\}} F(y)$ 
			\If{$F(y^*) \leq F(x)$}
				\State $x \gets y^*$
			\Else
				\State break
			\EndIf
		\EndWhile
		\State {\bf return} $x$, $F(x)$
	\end{algorithmic}
\end{algorithm}

Objectives $F(\cdot)$ for Max-Cut, Min UnCut and Sparsest Cut can be taken from 
problems \ref{maxcut1}, \ref{minuncut} and \ref{spcut1}  respectively with addition a 
factor $-1$, if necessary.
It remains to describe, how we choose a neighbourhood $\mathcal N(x)$ of a point $x$.
For arbitrary $x \in \{-1, 1 \}^n$ define $\mathcal N(x) = \{ y : \| y - x \|_1 \leq 2 \}$.
In other words, we call $y$ a neighbour of $x$, if it differs from $x$ no more, than in one 
position.
Since initial point is generated randomly, one can run the greedy algorithm several times 
and choose the best value of the objective.

It is easy to observe, that in case of Max-Cut the greedy algorithm on each step chooses 
a vertex $v$, such that the number of edges from $v$ in the cut is less, than a number of 
edges from $v$ outside the cut.
Similarly in case of Min UnCut the greedy algorithm on each step chooses 
a vertex $v$, such that the number of edges from $v$ outside the cut is less, than a 
number of edges from $v$ in the cut.
Thus, in these cases the greedy algorithm gives a 2-approximation of optimal solution.





\section{SDP relaxations}

\subsection{SDP relaxation of Max - Cut problem}

As it was already written earlier the Max-Cut can be formulated as the problem \ref{maxcut1}. In such formulation it is not convex as $x$ can only be equal to $-1$ or to $1$. Let's notice that $\forall x \hookrightarrow x^TWx = \trace (W xx^T) $, therefore if we introduce $X = xx^T$ and $\mathbb{I}$ a matrix containing only ones as elements, then the whole optimisation problem can be rewritten in the following way
\[
    \begin{cases}
        \frac 14 \left(\trace(W \mathbb{I}) - \trace(W X)\right) \longrightarrow \max,  &\\
        \diag(X) = 1, &\\
        X \succeq 0, &\\
        \rank(X) = 1,  &\\
        X_{i,j} \in \{-1,1\} & \forall i, j.
    \end{cases}
\]

It is  still not a convex problem, but we will obtain one if the last two constraints are thrown away. Hence, the SDP relaxation of Max-Cut looks like
\begin{equation}
    \label{maxcut_rel}
    \begin{cases}
        \frac 14 \left(\trace(W \mathbb{I}) - \trace(W X)\right) \longrightarrow \max\\
        \diag(X) = 1,\\
        X \succeq 0.
    \end{cases}
\end{equation}

\subsection{Min-UnCut relaxation}

For Min-UnCut SDP relaxation we used the idea described in \cite{Arora}. In the lecture it was suggested to use new graph $G'(V', E')$. In order to construct it, new vertices $\{-1, \dots, -n \}$ should be added to the initial graph. We connect two vertices $i$ and $j$ with an edge in $G'$ if and only if $i$ and $-j$ or $-i$ and $j$ are connected with an edge in $G$. Then the initial problem of finding the cut such that the sum of weights of edges not in the cut was minimal is equivalent to the problem of finding the minimal symmetric cut in $G'$, i.e. $(S', T' = -S')$ and $S' \cup T' = V'$, where $S' \equiv \{ -i | ~i\in S'\} $. That is true if every cut $(S, T)$ in the graph $G$ corresponds to the cut $(S \cup (-T), (-S) \cup T)$ in $G'$ and on the other hand if we have a cut $(S', T')$ in $G'$, then one can easily get the cut in $G$, where $S = \{i|~ i \geq 0, ~i\in S'\}$ and $T = \{i| ~ i \geq 0, ~i\in T' \}$. We can exploit it and then our problem is equivalent to 
\[
\begin{cases}
    \frac 18 \left(\trace(\tilde W \mathbb{I}) - \trace(\tilde W X)\right) \longrightarrow \min,  &\\
        \diag(X) = 1, &\\
        X_{i, -i} = -1, &\\
        X \succeq 0, &\\
        \rank(X) = 1,  &\\
        X_{i,j} \in \{-1,1\} & \forall i, j.
\end{cases}
\]
Here we took into account the fact that in the extended graph the number of edges doubles, therefore the coefficient $\frac 18$ is present. $X_{ij}$ has the similar meaning as for Max-Cut problem, but $X_{ij} = -1$ if and only if vertices $i$ and $j$ are separated by the cut in $G'$. Moreover, in this statement
\[
\tilde W = \begin{pmatrix} O & W \\ W & O\end{pmatrix}
\]
where $W$ is the matrix of weights for $G$. At last, the third constraint binds our cut to be symmetric or to satisfy $(S', T' = -S')$. 

As in the previous case two last constraints prevent this problem from being convex, hence, getting rid of them, we will have the SDP relaxation for Min-UnCut problem.
\begin{equation}
    \label{minuncut_rel}
    \begin{cases}
        \frac 18 \left(\trace(\tilde W \mathbb{I}) - \trace( \tilde W X)\right) \longrightarrow \min,\\
        \diag(X) = 1,\\
        X_{i, -i} = -1,\\
        X \succeq 0.\\
    \end{cases}
\end{equation}

\subsection{Sparsest Cut relaxation}

%Basing on (\ref{spcut1}), TO BE DONE!!!!! 
Basing on (\ref{spcut1}), we can get the relaxed problem by properly dealeing with the denominator. We want to find $t$, such that $t\Biggl\{ n^2 - \left( \sum\limits_{i=1}^n x_i \right)^2\Biggr\} = 1$. Then introducing as in the previous cases the matrix $X$ with elements $X_{ij} = x_i x_j$ and also a new vector $y = \sqrt{t} x$ and $Y = \{Y_{ij}\}_{i,j = 1}^n$, where $Y_{ij} = y_iy_j = tx_ix_j = tX_{ij}$, one can see that the problem of finding the sparsest cut is equivalent to the following
\[
\begin{cases}
   t \sum\limits_{i,j=1}^n a_{ij} -  \trace(AY) \longrightarrow \min, &\\
   t n^2 - \sum\limits_{i,j=1}^n Y_{ij}  = 1, &\\
   \diag(Y) = t, & \\
   t > 0, &\\
   \rank(Y), &\\
   Y_{ij} \in \{\sqrt{t}, -\sqrt{t}\} & \forall i,j.
\end{cases}
\]
Here, as in the cases listed above two last constraints aren't suitable for SDP programming, therefore they should be thrown away. Furthermore, the first constraint can be replaced by $t\Bigl\{ n^2 - \sum\limits_{i,j=1}^n X_{ij} \Bigr\} \geq 1$ equivalently. Finally, in order to make our relaxation be narrower, one may notice that as the number $\Biggl\{ n^2 - \left( \sum\limits_{i=1}^n x_i \right)^2\Biggr\}$ corresponds to the value $|S||V\backslash S|$. There are known upper and lower bounds for this value. The upper bound is $\dfrac{n^2}4$ if $n$ is an even number and $\dfrac{n^2-1}4$ if $n$ is odd. The lower bound is $n-1$. Taking into account all the mentioned, we obtain the SDP relaxation for our problem:
\begin{equation}
    \label{spcut_rel}
    \begin{cases}
       t \sum\limits_{i,j=1}^n a_{ij} -  \trace(AY) \longrightarrow \min, &\\
   t n^2 - \sum\limits_{i,j=1}^n Y_{ij} \geq 1, \\
   \diag(Y) = t,  \\
    \dfrac1{4(n-1)} \geq t \geq \dfrac1{n^2}. \\
    \end{cases}
\end{equation}


\subsection{Relaxations with addition of triangle constraints}

If we introduce the metric for our partitioning tasks:
\[
d(i,j) = \begin{cases}
    0 & \text{if $i$ and $j$ are vertices in the same part of the graph,}\\
    1 & \text{otherwise,}
\end{cases}
\]
which satisfies all the metric conditions, then the variables in the problems mentioned above are connected with the metric in the following way
\[
X_{ij} = 1 - 2d(i,j).
\]
Taking into account this connection, one can obtain the triangle constraints with our variables, writing those for the metric itself:
\[
d(i,j) + d(j, k) \geq d(i,k) \Leftrightarrow X_{ij} + X_{jk} - 1 \leq X_{ik}. 
\]
Those expressions should be true for any $i$, $j$ and $k$. Let's denote for any $j$ $E_j = (\mathbf{e}_j, \dots, \mathbf{e}_j)$ and $\mathbf{e}_j$ is a unit vector with 1 on the $j$-th place. From this follows that the constraint above can be rewritten as following:
\[
XE_j + E_j^T X - 1 \leq X ~~~~~~~~~~~ \forall j. 
\]

Beside all the mentioned above, one could notice that for the metric $d(i,j)$ it also holds the fact that
\[
d(i,j) + d(j,k) + d(i,k) \leq 2 ~~~~~~~~~~~ \forall i, j, k.
\]
Therefore, applying the link between the metric and $X$, we have
\[
X_{ij} + X_{jk} + X_{ik} \geq -1 ~~~~~~~~~~~ \forall i, j, k.
\]
Rewriting it in the matrix way, we will derive the last triangle constraint:
\[
XE_j + E_j^T X + X \geq -1 ~~~~~~~~~~~ \forall j.
\]
As for the SDP relaxation of the sparsest cut problem, in our problem statement there are variables $Y$ and $t$ and $Y$ is connected with $X$ by $Y = tX$ expression. Hence, the formulas presented above should be rewritten for the sparsest cut problem in the following way
\begin{align*}
    YE_j + E_j^T Y + Y &\geq -t  &\forall j \\
    YE_j + E_j^T Y - t &\leq Y  &\forall j
\end{align*}
However, due to the slow work of this SDP relaxation, we aren't going to include its results in the report.



\section{LP relaxations}

\subsection{LP relaxation of Max - Cut problem}

We first design an integer program for MAXCUT and then relax it to get a LP.

\textbf{Integer Program Version} Define variables $x_u, u \in V \ $ and $\ e_{uv}, (u,v) \in E$ as follows
which are supposed to imply the following.

\begin{equation}
e_{uv} =
\label{primal}
	\begin{cases}
		1 \quad  \text{if} \; (u, v) \; \text{is in cut} \\
		0 \quad \text{otherwise} 
	\end{cases}
\end{equation}

\begin{equation}
x_u =
\label{primal}
	\begin{cases}
		1 \quad  \text{if} \; u \in S\\
		0 \quad \text{otherwise} 
	\end{cases}
\end{equation}

MAXCUT can now be phrased as the following integer program.

\begin{equation}
    \begin{cases}
     \sum\limits_{(u,v) \in E} e_{uv} \longrightarrow \max \\
	
		e_{uv} \le \begin{cases}
		x_u + x_v \\
		2 - (x_u + x_v)
	\end{cases}
	\; \forall (u,v) \in E\\

	x_u \in \{0,1\} \quad \forall u \in V\\
	
	e_{uv} \in \{0,1\} \quad \forall (u,v) \in E
    \end{cases}
\end{equation}

Notice that $e_{uv} \ne 0 \Longleftrightarrow x_u \ne x_v$.

We now relax $e_{uv} \in \{0,1\} \ \text{to} \ 0 \le e_{uv} \le 1$ and $x_{uv} \in \{ 0,1\}$ to $0 \le x_{uv} \le 1$ to obtain the following LP relaxation.

\begin{equation}
    \begin{cases}
    \sum\limits_{(u,v) \in E} e_{uv} \longrightarrow \max \\
	
		e_{uv} \le \begin{cases}
		x_u + x_v \\
		2 - (x_u + x_v)
	\end{cases}
	\; \forall (u,v) \in E\\

	x_u \in [0,1] \quad \forall u \in V\\
	
	e_{uv} \in [0,1] \quad \forall (u,v) \in E
    \end{cases}
\end{equation}

We can now solve the above LP using an LP-solver. Every solution to the Integer
Program is also a solution to the Linear Program. 

\textbf{LP relaxation for MaxCut:}

\begin{equation}
    \begin{cases}
     \frac14 \vec 1^T W \vec 1 - \frac14 x^T W x \longrightarrow \max \\
	
		e_{uv} \le \begin{cases}
		x_u + x_v \\
		2 - (x_u + x_v)
	\end{cases}
	\; \forall (u,v) \in E\\

	x_u \in [0,1] \quad \forall u \in V\\
	
	e_{uv} \in [0,1] \quad \forall (u,v) \in E
    \end{cases}
\end{equation}

\subsection{LP relaxation of Min-UnCut problem}

$\quad \ \ $For Min-UnCut SDP relaxation we used the idea described in \cite{Arora} and section $3.2$.

Min UnCut is equivalent to the problem
of finding a minimum symmetric cut $(S, \bar{S} = -S) \ \text{in} \ G$.

\textbf{LP relaxation for Min-UnCut:}

\begin{equation}
\begin{cases}
    \frac18 \vec 1^T \tilde W \vec 1 + \frac18 x^T \tilde W x \longrightarrow \min \\
        	e_{uv} \ge \begin{cases}
		x_u + x_v \\
		2 - (x_u + x_v)
	\end{cases}
	\; \forall (u,v) \in E\\

	x_u \in [0,1] \quad \forall u \in V\\
	
	e_{uv} \in [0,1] \quad \forall (u,v) \in E
\end{cases}
\end{equation}

where

\[
\tilde W = \begin{pmatrix} O & W \\ W & O\end{pmatrix}
\]

\subsection{LP relaxation for Sparsest Cut:}

The sparsest cut problem seeks to find
\begin{equation}
    \Phi^*= \min\limits_{S \subseteq V} \frac{\bar{w} \ \bar{\delta}_S} {\bar{D} \ \bar{\delta}_S} 
\end{equation}
where $\delta_{S_{ij}}$ - cut metric
\begin{equation}
    \delta_{S_{ij}} = 
    \begin{cases}
        0 \quad \text{if} \ i, j \in S \ \text{or} \ i,j \in \bar{S}\\
        1 \quad \text{otherwise}
    \end{cases}
\end{equation}

non-negative demands $D_{ij}$ between every pair of vertices (in our case $D_{ij} = 1$)

non-negative edge costs (or capacities) $w_{ij}$ between every pair of vertices


To form the LP relaxation, we relax the requirement of minimizing over the cut metrics to minimizing
over all metrics. That is,
\begin{equation}
    \lambda^*= \min\limits_{\mathrm{metrics} \ \mathrm{d}} \frac{\bar{w} \ \bar{d}} {\bar{D} \ \bar{d}} 
\end{equation}

The above relaxation is solved by the following linear program:

\begin{equation}
    \begin{cases}
     \sum\limits_{i,j} w_{ij} d_{ij} \longrightarrow \min \\
	\sum\limits_{i,j} D_{ij} d_{ij} = 1\\
	d_{ij} + d_{jk} \ge d_{ik} \quad \forall i,j,k\\
	d_{ij} \ge 0 \quad \forall i,j
    \end{cases}
\end{equation}

\section{Gradient descent}

Consider a SDP problem

\begin{equation}
\label{primal}
	\begin{cases}
		\text{Tr} \left( W^{\top} X \right) \longrightarrow \min \\
		\text{diag} \left( X \right) = 1\\
		X \succeq 0,
	\end{cases}
\end{equation}
Let's reformulate this problem
\begin{equation}
\label{primal}
	\begin{cases}
		\text{Tr} \left( W^{\top} X \right) - \mu \log \text{det} X \longrightarrow \min \\
		\text{diag} \left( X \right) = 1,
	\end{cases}
\end{equation}
Gradient
\[
df = \text{Tr} \left[ (W - \mu X^{-1}) \delta X\right]
\]
\[
\nabla f = (W - \mu X^{-1})
\]
Step
\[
\Delta X = \pi_L \left( \alpha  (W - \mu X^{-1})\right) =  \alpha \pi_L \left(  W - \mu X^{-1}\right)
\]
for MaxCut problem 
\[L = \{z \ | \ z_{ii} = 0 \quad \forall i\}\]
for Min UnCut problem \[L = \{z \ | \ z_{ii} = 0, z_{i,n+i} = 0, z_{n+i, i} = 0 \quad \forall i\}\]












\section{Primal-dual central path tracing interior point method}

Consider an SDP problem
\begin{equation}
\label{primal}
	\begin{cases}
		\langle C, X \rangle \longrightarrow \min \\
		X \in \mathcal L - B \\
		X \succeq 0,
	\end{cases}
\end{equation}
where $\langle C, X \rangle  = \text{Tr}(C^T X)$.
The dual problem can be formulated as follows
\begin{equation}
\label{dual}
	\begin{cases}
		\langle B, S \rangle \longrightarrow \max \\
		S \in \mathcal L^\perp + C \\
		S \succeq 0
	\end{cases}
\end{equation}
If problems \ref{primal} and \ref{dual} are strictly feasible, then the strong duality takes 
place.
We describe an interior point method based on primal-dual central path tracing.
Consider a pair of modified primal and dual problems
\begin{equation}
	X_\mu = \arg\min\limits_{X \in \mathcal L} \langle C, X \rangle - \mu \log\det X
\end{equation}
\begin{equation}
	S_\mu = \arg\max\limits_{S \in \mathcal L^\perp} \langle B, X \rangle + \mu \log\det S
\end{equation}
It is known, that $X_\mu S_\mu = S_\mu X_\mu = I$, and duality gap will be equal to 
$\langle X_\mu, S_\mu \rangle = \mu n$.
The set $\left\{ (X_\mu, S_\mu) : \mu > 0 \right\}$ defines a so-called primal-dual central 
path.
If one can move along the primal-dual central path, then one can easily solve 
optimization problems \ref{primal}, \ref{dual} with arbitrary predefined accuracy tending 
$\mu$ to zero.
Unfortunately, we cannot stay on the primal-dual central path, but we can try to move in 
its vicinity.
The following algorithm allows to trace the primal-dual central path.
\begin{algorithm}[H]
	\caption{Primal-dual central path tracing IPM}
	\label{ipm}
	\begin{algorithmic}[1]
		\Require Matrices $B$ and $C$ as in problems \ref{primal}, \ref{dual};
		projectors $\pi_{\mathcal L}(\cdot, Q)$, $\pi_{\mathcal L^\perp}(\cdot, Q)$ on 
		linear spaces $Q \mathcal L Q$ and $Q^{-1} \mathcal L^\perp Q^{-1}$;
		parameters $0 \leq \chi \leq \kappa \leq 0.1$;
		initial values $X \in \mathcal L - B$, $S \in \mathcal L^\perp + C$, $X, S \succ 
		0$ 
		and $t$, which fulfil
		\[	
			\| t X^{\frac12} S X^{\frac12} - I \|_2 \leq \kappa;
		\]
		tolerance $\varepsilon$ and (or) number of iterations $N_{\max}$
		\Ensure $\varepsilon$-optimal solution $X$
		\State Compute duality gap $\langle X, S \rangle$
		\While{$(\text{Duality gap} > \varepsilon)$ or $(\text{number of iterations} < 
		N_{\max})$}
			\State $t \gets \left( 1 - \frac\chi{\sqrt{k}} \right)^{-1} t$
			\State Choose a Nesterov-Todd scaling matrix
			\begin{gather}
			\label{nt}
				 Q = P^\frac12 \\
			\label{nt1}
				 P = X^{-\frac12} \left( X^{-\frac12} S^{-1} X^{-\frac12} \right)^\frac12 
				 X^\frac12 S
			\end{gather}
			\State $\hat X \gets Q X Q$
			\State Solve a Lyapunov matrix equation
			\begin{equation}
			\label{lyapunov}
				\hat X Z + Z \hat X = \frac2t I - 2 \hat X^2
			\end{equation}
			\State $\Delta \hat X \gets \pi_{\mathcal L}(Z, Q)$, $\Delta \hat S \gets 
			\pi_{\mathcal 
			L^\perp}(Z, Q)$
			\State $\Delta X \gets Q^{-1} \Delta \hat X Q^{-1}$, $\Delta S \gets Q 
			\Delta\hat S Q$
			\State Update $X \gets X + \Delta X$, $S \gets S + \Delta S$, $t \gets \frac 
			k{\langle X, S \rangle}$
			\State Compute duality gap $\langle X, S \rangle = \frac kt$ 
		\EndWhile
		\State {\bf return} $X$
	\end{algorithmic}
\end{algorithm}
The Nesterov-Todd scaling matrix defined by \ref{nt} -- \ref{nt1} ensures, that matrices 
$\hat X = Q 
X Q$ and $\hat S = Q^{-1} S Q^{-1}$ commute and moreover $\hat X = \hat S$. 
The equation \ref{lyapunov} can be considered as a linearization of the equation
\[
	\tilde X \tilde S + \tilde S \tilde X = \frac2t I
\]
at the point $(\hat X, \hat S) = (QXQ, Q^{-1} S Q^{-1})$ w.r.t. $X$ and $S$, and $Z = Q 
\Delta X Q + Q^{-1} \Delta S Q^{-1}$.
Note, that the algorithm guarantees, that $\Delta X \in \mathcal L$ and $\Delta S \in 
\mathcal L^\perp$, so the output $X$ is feasible.
We refer to lectures of Ben-Tal and Nemirovski \cite{nemirovski} for more details.

The next theorem guarantees consistency of the procedure \ref{ipm}.
\newtheorem{Th}{Theorem}
\begin{Th}{4.5.2 in \cite{nemirovski}}
	Let the parameters $\chi$, $\kappa$ of \ref{ipm} satisfy
	\[
		0 < \chi \leq \kappa \leq 0.1
	\]
	Let $(X, S)$ be a pair of strictly feasible primal and dual solutions to \ref{primal}, 
	\ref{dual}, such that the triple $(X, S, t)$ satisfies
	\begin{equation}
	\label{2}
		\| t X^{\frac12} S X^{\frac12} - I \|_2 \leq \kappa;
	\end{equation}
	Then the updated pair $(X + \Delta X, S + \Delta S)$ is well-defined (i. e. equation 
	\ref{lyapunov} has a unique solution), $X + \Delta X$, $S + \Delta S$ are strictly 
	feasible solutions to \ref{primal}, \ref{dual} respectively, and the triple $\left(X + 
	\Delta X, S + \Delta S, t = \frac k{ \langle X, S \rangle } \right)$ satisfies \ref{2}.
\end{Th}
	
\renewcommand{\refname}{References}
\begin{thebibliography}{0}
	\bibitem{Arora}
		Sanjeev Arora, {\it Lecture 8: Approximating Min UnCut and Min-2CNF
Deletion}, (2005)
    
    \verb'http://www.cs.princeton.edu/courses/archive/spr05/cos598B/lecture8.pdf'
	
	\bibitem{nemirovski}
	A. Ben-Tal, A. Nemirovski, {\it Lectures on Modern Convex Optimization}, (2013)\\ 
	\verb'http://www2.isye.gatech.edu/~nemirovs/Lect_ModConvOpt.pdf'
\end{thebibliography}
\end{document}
